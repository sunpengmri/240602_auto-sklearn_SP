{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 方差阈值法\n",
    "1. 方差阈值法是一种简单的过滤方法，它通过移除方差小于给定阈值的特征来减少数据集的维度。方差低的特征通常对模型的区分能力贡献不大，特别是在分类任务中。原理方差是衡量数据分布离散程度的一个指标。对于离散程度非常小的特征（即方差接近于0），在分类问题中几乎不提供任何有用的信息。因此，这些特征可以被安全地移除。核心公式其中， 是第  个特征的方差， 是第  个样本在特征  上的取值， 是特征  的均值。推导：1. 均值计算：首先计算特征  的均值。\n",
    "2. 方差计算：接着，计算每个样本值与均值的偏差的平方，并求平均值。\n",
    "3. 方差比较：如果  小于设定的阈值，则剔除特征 。\n",
    "4. 阈值设置：阈值可以根据经验设定或通过交叉验证调整，通常选择一个较小的正数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 设置随机种子以保证结果可重复\n",
    "np.random.seed(42)\n",
    "\n",
    "# 生成虚拟数据集\n",
    "n_samples = 1000\n",
    "n_features = 5\n",
    "\n",
    "# 数据分布：0~1均匀分布\n",
    "X = np.random.rand(n_samples, n_features)\n",
    "\n",
    "# 人为地将某些特征的方差设置为接近0（即无变化）\n",
    "X[:, 1] = 0.5  # 第二个特征方差为0\n",
    "X[:, 3] = X[:, 3] * 0.01  # 第四个特征方差非常小\n",
    "\n",
    "# 创建DataFrame以便查看数据\n",
    "columns = [f'Feature_{i+1}' for i in range(n_features)]\n",
    "df = pd.DataFrame(X, columns=columns)\n",
    "\n",
    "# 方差阈值法\n",
    "threshold = 0.01  # 设置阈值\n",
    "selector = VarianceThreshold(threshold=threshold)\n",
    "X_selected = selector.fit_transform(X)\n",
    "\n",
    "# 保留的特征\n",
    "retained_features = df.columns[selector.get_support()]\n",
    "dropped_features = df.columns[~selector.get_support()]\n",
    "\n",
    "# PCA降维到2D以便可视化\n",
    "pca = PCA(n_components=2)\n",
    "X_pca_before = pca.fit_transform(X)\n",
    "X_pca_after = pca.fit_transform(X_selected)\n",
    "\n",
    "# 创建图形\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10), constrained_layout=True)\n",
    "fig.suptitle('Variance Threshold Feature Selection', fontsize=16)\n",
    "\n",
    "# 原始数据的散点图 (PCA降维到2D)\n",
    "axes[0, 0].scatter(X_pca_before[:, 0], X_pca_before[:, 1], c='blue', edgecolor='k', s=50, alpha=0.7)\n",
    "axes[0, 0].set_title('Original Data (PCA 2D)', fontsize=14)\n",
    "axes[0, 0].set_xlabel('PCA Component 1')\n",
    "axes[0, 0].set_ylabel('PCA Component 2')\n",
    "\n",
    "# 方差阈值法后的数据散点图 (PCA降维到2D)\n",
    "axes[0, 1].scatter(X_pca_after[:, 0], X_pca_after[:, 1], c='green', edgecolor='k', s=50, alpha=0.7)\n",
    "axes[0, 1].set_title('After Variance Threshold (PCA 2D)', fontsize=14)\n",
    "axes[0, 1].set_xlabel('PCA Component 1')\n",
    "axes[0, 1].set_ylabel('PCA Component 2')\n",
    "\n",
    "# 原始数据的热力图\n",
    "im1 = axes[1, 0].imshow(np.corrcoef(X.T), cmap='viridis', aspect='auto')\n",
    "axes[1, 0].set_title('Original Data Correlation Heatmap', fontsize=14)\n",
    "axes[1, 0].set_xticks(np.arange(n_features))\n",
    "axes[1, 0].set_yticks(np.arange(n_features))\n",
    "axes[1, 0].set_xticklabels(columns, rotation=45)\n",
    "axes[1, 0].set_yticklabels(columns)\n",
    "fig.colorbar(im1, ax=axes[1, 0])\n",
    "\n",
    "# 方差阈值法后的数据热力图\n",
    "im2 = axes[1, 1].imshow(np.corrcoef(X_selected.T), cmap='viridis', aspect='auto')\n",
    "axes[1, 1].set_title('After Variance Threshold Correlation Heatmap', fontsize=14)\n",
    "axes[1, 1].set_xticks(np.arange(len(retained_features)))\n",
    "axes[1, 1].set_yticks(np.arange(len(retained_features)))\n",
    "axes[1, 1].set_xticklabels(retained_features, rotation=45)\n",
    "axes[1, 1].set_yticklabels(retained_features)\n",
    "fig.colorbar(im2, ax=axes[1, 1])\n",
    "\n",
    "# 显示图形\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 相关系数法\n",
    "1. 相关系数法用来衡量特征与目标变量之间的线性关系。该方法适用于数值型特征与目标变量的关系分析。相关系数法通常用于特征筛选的早期阶段，以过滤掉与目标变量相关性较低的特征。\n",
    "\n",
    "2. 原理: 相关系数（如皮尔逊相关系数）度量的是两个变量之间的线性相关性。值的范围从 -1 到 1，分别表示完全负相关和完全正相关，0 则表示无线性相关性。核心公式（皮尔逊相关系数）：其中， 和  分别是特征  和目标变量  的样本值， 和  是它们的均值。\n",
    "\n",
    "3. 推导：1. 计算均值：首先，计算特征和目标变量的均值。2. 协方差计算：计算特征和目标变量之间的协方差。3. 标准差计算：计算特征  和目标变量  的标准差。4. 相关系数计算：最后，通过协方差和标准差的比值计算相关系数。\n",
    "\n",
    "4. 选择相关系数较大的特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# 设置随机种子以确保结果可复现\n",
    "np.random.seed(42)\n",
    "\n",
    "# 生成虚拟数据集\n",
    "# Ensure 'Target' column is added to the dataframe\n",
    "df['Target'] = np.random.rand(n_samples)  # Example target values, replace with actual target data\n",
    "\n",
    "columns = [f'Feature_{i+1}' for i in range(n_features)]\n",
    "correlations = {}\n",
    "for col in columns:\n",
    "    corr, _ = pearsonr(df[col], df['Target'])\n",
    "    correlations[col] = corr\n",
    "\n",
    "# 将相关系数结果转换为DataFrame\n",
    "corr_df = pd.DataFrame.from_dict(correlations, orient='index', columns=['Correlation'])\n",
    "\n",
    "# 绘制图形\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "# 图1：相关系数矩阵的热图\n",
    "plt.subplot(2, 1, 1)\n",
    "corr_matrix = df.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, linewidths=0.5)\n",
    "plt.title('Correlation Matrix Heatmap', fontsize=16)\n",
    "\n",
    "# 图2：特征与目标变量的散点图\n",
    "plt.subplot(2, 1, 2)\n",
    "colors = sns.color_palette('husl', n_features)\n",
    "for i, col in enumerate(columns):\n",
    "    plt.scatter(df[col], df['Target'], color=colors[i], alpha=0.7, label=f'{col} (Corr: {correlations[col]:.2f})')\n",
    "\n",
    "plt.xlabel('Feature Value', fontsize=12)\n",
    "plt.ylabel('Target Value', fontsize=12)\n",
    "plt.title('Scatter Plot of Features vs. Target', fontsize=16)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 递归特征消除\n",
    "递归特征消除（RFE）是一种迭代方法，基于模型的特征重要性逐步减少特征数目。RFE 通过每次训练模型并移除重要性最低的特征，最终找到性能最佳的特征子集。\n",
    "\n",
    "原理RFE 的核心思想是通过反复训练模型，评估每个特征的重要性，并在每一轮中移除最不重要的特征。最终剩下的特征就是被认为最重要的特征子集。核心公式RFE 不依赖具体的公式，而是基于模型计算的特征重要性得分（如权重或特征贡献度）。例如在线性回归中，使用回归系数的绝对值作为特征的重要性度量。\n",
    "\n",
    "推导：1. 训练初始模型：使用所有特征训练模型，计算特征的重要性。2. 移除不重要特征：从模型中移除权重绝对值最小的特征（在其他模型中则可能是信息增益、基尼系数等）。3. 递归：重复步骤1和2，直到达到所需的特征数量。4. 最终特征选择：选择在模型性能评估中表现最好的特征子集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# 生成虚拟数据集\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=5, n_classes=2, random_state=42)\n",
    "\n",
    "# 拆分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 初始化分类器\n",
    "svc = SVC(kernel=\"linear\")\n",
    "\n",
    "# 初始化RFE，选择最优特征数\n",
    "rfe = RFE(estimator=svc, n_features_to_select=5)\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# 绘制特征排序\n",
    "ranking = rfe.ranking_\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(X.shape[1]), ranking, color='dodgerblue')\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Ranking\")\n",
    "plt.title(\"Feature Ranking using RFE\")\n",
    "plt.xticks(range(X.shape[1]), labels=range(1, X.shape[1]+1))\n",
    "\n",
    "# 计算不同特征数量下的模型性能\n",
    "scores = []\n",
    "num_features = range(1, X.shape[1] + 1)  # Adjusted range to match the length of scores\n",
    "for n in num_features:\n",
    "    rfe = RFE(estimator=svc, n_features_to_select=n)\n",
    "    rfe.fit(X_train, y_train)\n",
    "    score = np.mean(cross_val_score(rfe, X_train, y_train, cv=5))\n",
    "    scores.append(score)\n",
    "\n",
    "# 绘制特征数量与模型性能的关系\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(num_features, scores, marker='o', color='crimson', linestyle='-', linewidth=2, markersize=8)\n",
    "plt.xlabel(\"Number of Selected Features\")\n",
    "plt.ylabel(\"Cross-Validated Accuracy\")\n",
    "plt.title(\"Model Performance vs. Number of Features\")\n",
    "plt.grid(True)\n",
    "\n",
    "# 显示图像\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. L1正则化\n",
    "1.  Lasso回归是一种线性回归模型，通过在损失函数中加入L1正则化项，促使一些回归系数变为零，从而实现特征选择。Lasso的稀疏性特性使其在处理高维数据时特别有效。\n",
    "\n",
    "2. 原理: L1正则化会对回归系数施加一个稀疏性约束，使得部分系数被压缩为零。这意味着模型只会使用部分特征，从而实现特征选择的效果。核心公式其中， 是正则化强度， 是特征  的回归系数。\n",
    "\n",
    "3. 推导：1. 最小化损失函数：目标是找到一组回归系数  ，使得损失函数最小化。2. L1正则化引入：通过引入L1正则化项 ，在回归过程中自动选择特征。3. 梯度下降法：通常使用梯度下降法进行优化。由于L1正则化的非光滑性，可以通过次梯度法或坐标下降法求解。\n",
    "\n",
    "4. 特征选择：随着的增加，更多的将被压缩为零，从而选择出对模型贡献最大的特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 生成虚拟数据集\n",
    "np.random.seed(0)\n",
    "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=0)\n",
    "\n",
    "# 添加一些无关特征\n",
    "X = np.hstack([X, np.random.randn(X.shape[0], 5)])\n",
    "\n",
    "# 拆分数据集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# 设置Lasso回归模型\n",
    "alphas = [0.01, 0.1, 1, 10, 100]\n",
    "coefs = []\n",
    "mse_train = []\n",
    "mse_test = []\n",
    "\n",
    "# 训练Lasso回归模型并记录系数和均方误差\n",
    "for alpha in alphas:\n",
    "    model = Lasso(alpha=alpha, max_iter=10000)\n",
    "    model.fit(X_train, y_train)\n",
    "    coefs.append(model.coef_)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    mse_train.append(mean_squared_error(y_train, y_train_pred))\n",
    "    mse_test.append(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "# 绘制结果图\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# 系数图\n",
    "plt.subplot(1, 2, 1)\n",
    "for i in range(len(alphas)):\n",
    "    plt.plot(coefs[i], label=f'Alpha={alphas[i]}', marker='o')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('Lasso Coefficients vs. Feature Index')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# 均方误差图\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(alphas, mse_train, label='Train MSE', marker='o', color='blue')\n",
    "plt.plot(alphas, mse_test, label='Test MSE', marker='o', color='red')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Alpha (Regularization Strength)')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('MSE vs. Alpha')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 基于树模型的特征选择\n",
    "\n",
    "1. 树模型（如决策树、随机森林、XGBoost）可以通过其内部结构自动评估特征的重要性。通过统计每个特征在树中的分裂贡献，生成特征重要性分数，并根据这些分数进行特征选择。\n",
    "\n",
    "2. 原理: 树模型在构建过程中，通过特征选择来进行节点分裂。某个特征的重要性可以通过其在所有树中的累积贡献来衡量。这种方法对于处理高维数据和非线性关系特别有效。\n",
    "\n",
    "3. 核心公式特征重要性可以通过累积的分裂点贡献度来计算。对于决策树：其中， 表示特征  的重要性， 是第  个节点由特征  分裂带来的信息增益。\n",
    "\n",
    "4. 推导：1. 训练树模型：使用训练数据构建树模型，如随机森林或决策树。2. 计算节点分裂信息增益：在每次分裂时，计算信息增益或基尼指数的变化。3. 累积重要性：累积每个特征在所有树中的贡献度，即特征重要性。4. 排序和选择：根据特征重要性得分排序，并选择得分较高的特征。Python实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# 生成虚拟数据集\n",
    "X, y = make_classification(n_samples=500, n_features=20, n_informative=15, n_redundant=5, random_state=42)\n",
    "\n",
    "# 标准化数据\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 使用随机森林分类器进行特征选择\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_scaled, y)\n",
    "\n",
    "# 获取特征重要性\n",
    "importances = clf.feature_importances_\n",
    "\n",
    "# 基于特征重要性选择特征\n",
    "selector = SelectFromModel(clf, threshold=\"mean\", prefit=True)\n",
    "X_selected = selector.transform(X_scaled)\n",
    "\n",
    "# 绘制特征重要性图和选择后特征图\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# 特征重要性图\n",
    "plt.subplot(1, 2, 1)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "plt.bar(range(X_scaled.shape[1]), importances[indices], color='skyblue', align='center')\n",
    "plt.xticks(range(X_scaled.shape[1]), indices + 1)\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importance from Random Forest')\n",
    "\n",
    "# 选择后的特征图\n",
    "plt.subplot(1, 2, 2)\n",
    "selected_features = np.sum(selector.get_support())\n",
    "plt.bar([0], [selected_features], color='salmon', align='center')\n",
    "plt.xticks([0], ['Selected Features'])\n",
    "plt.xlabel('Selected Features')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Number of Selected Features')\n",
    "\n",
    "# 显示图形\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 卡方检验\n",
    "\n",
    "1. 卡方检验主要用于检测分类特征和分类目标变量之间的独立性。它适合于离散型数据，尤其是在处理分类问题时，可以用来评估每个特征与目标变量的关联性。\n",
    "\n",
    "2. 原理: 卡方检验计算的是观察频率和期望频率之间的差异，以此判断两个分类变量是否独立。如果独立性假设被拒绝，则说明特征和目标变量之间存在显著关联。核心公式其中， 是第  类的观察频率， 是第  类的期望频率。\n",
    "\n",
    "3. 推导：1. 构建列联表：根据特征和目标变量的不同取值，构建观测频率表（即列联表）。2. 计算期望频率：其中， 是第  行的总频数， 是第  列的总频数， 是总样本数。 3. 计算卡方统计量：使用卡方公式计算观测频率与期望频率之间的差异。4. 卡方检验：将计算的卡方统计量与卡方分布表对比，确定p值。如果p值小于设定的显著性水平，则拒绝独立性假设，表明该特征对目标变量有显著影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X must be non-negative.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m y_encoded \u001b[38;5;241m=\u001b[39m le\u001b[38;5;241m.\u001b[39mfit_transform(y_df)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# 卡方检验\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m X_new \u001b[38;5;241m=\u001b[39m \u001b[43mSelectKBest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchi2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mall\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_encoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m chi2_values, p_values \u001b[38;5;241m=\u001b[39m chi2(X, y_encoded)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# 创建图形\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/autorad/lib/python3.10/site-packages/sklearn/utils/_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 142\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    147\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    148\u001b[0m         )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/autorad/lib/python3.10/site-packages/sklearn/utils/_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 142\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    147\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    148\u001b[0m         )\n",
      "    \u001b[0;31m[... skipping similar frames: _wrap_method_output.<locals>.wrapped at line 142 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/autorad/lib/python3.10/site-packages/sklearn/utils/_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 142\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    147\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    148\u001b[0m         )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/autorad/lib/python3.10/site-packages/sklearn/base.py:851\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m--> 851\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/autorad/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:472\u001b[0m, in \u001b[0;36m_BaseFilter.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    467\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    468\u001b[0m     X, y, accept_sparse\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m], multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    469\u001b[0m )\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params(X, y)\n\u001b[0;32m--> 472\u001b[0m score_func_ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(score_func_ret, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscores_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpvalues_ \u001b[38;5;241m=\u001b[39m score_func_ret\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/autorad/lib/python3.10/site-packages/sklearn/feature_selection/_univariate_selection.py:217\u001b[0m, in \u001b[0;36mchi2\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m    215\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(X, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m(np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many((X\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;28;01melse\u001b[39;00m X) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m--> 217\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput X must be non-negative.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# Use a sparse representation for Y by default to reduce memory usage when\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# y has many unique classes.\u001b[39;00m\n\u001b[1;32m    221\u001b[0m Y \u001b[38;5;241m=\u001b[39m LabelBinarizer(sparse_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mfit_transform(y)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X must be non-negative."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# 生成虚拟数据集\n",
    "np.random.seed(42)\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_classes=3, n_informative=5, n_redundant=0, random_state=42)\n",
    "\n",
    "# 将特征转化为数据框\n",
    "feature_names = [f'Feature_{i}' for i in range(X.shape[1])]\n",
    "X_df = pd.DataFrame(X, columns=feature_names)\n",
    "y_df = pd.Series(y, name='Target')\n",
    "\n",
    "# 合并特征和目标变量\n",
    "data = pd.concat([X_df, y_df], axis=1)\n",
    "\n",
    "# 转换目标变量为类别标签\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y_df)\n",
    "\n",
    "# 卡方检验\n",
    "X_new = SelectKBest(chi2, k='all').fit_transform(X, y_encoded)\n",
    "chi2_values, p_values = chi2(X, y_encoded)\n",
    "\n",
    "# 创建图形\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# 特征卡方检验值的条形图\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.barplot(x=feature_names, y=chi2_values, palette='viridis')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Chi-Square Values for Each Feature')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Chi-Square Value')\n",
    "\n",
    "# 特征p值的条形图\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.barplot(x=feature_names, y=p_values, palette='plasma')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('P-Values for Each Feature')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('P-Value')\n",
    "\n",
    "# 特征值分布直方图\n",
    "plt.subplot(2, 2, 3)\n",
    "for feature in feature_names:\n",
    "    sns.histplot(data[feature], kde=True, label=feature, bins=30)\n",
    "plt.legend()\n",
    "plt.title('Distribution of Feature Values')\n",
    "plt.xlabel('Feature Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# 目标变量的频率分布直方图\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.histplot(y_df, kde=True, bins=3, palette='coolwarm')\n",
    "plt.title('Distribution of Target Variable')\n",
    "plt.xlabel('Target Class')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autorad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
