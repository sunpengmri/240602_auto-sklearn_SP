{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Parallel Usage: Spawning workers from the command line\n",
        "\n",
        "*Auto-sklearn* uses\n",
        "[dask.distributed](https://distributed.dask.org/en/latest/index.html)\n",
        "for parallel optimization.\n",
        "\n",
        "This example shows how to start the dask scheduler and spawn\n",
        "workers for *Auto-sklearn* manually from the command line. Use this example\n",
        "as a starting point to parallelize *Auto-sklearn* across multiple\n",
        "machines.\n",
        "\n",
        "To run *Auto-sklearn* in parallel on a single machine check out the example\n",
        "`sphx_glr_examples_60_search_example_parallel_n_jobs.py`.\n",
        "\n",
        "If you want to start everything manually from within Python\n",
        "please see ``:ref:sphx_glr_examples_60_search_example_parallel_manual_spawning_python.py``.\n",
        "\n",
        "**NOTE:** Above example is disabled due to issue https://github.com/dask/distributed/issues/5627\n",
        "\n",
        "\n",
        "You can learn more about the dask command line interface from\n",
        "https://docs.dask.org/en/latest/setup/cli.html.\n",
        "\n",
        "When manually passing a dask client to Auto-sklearn, all logic\n",
        "must be guarded by ``if __name__ == \"__main__\":`` statements! We use\n",
        "multiple such statements to properly render this example as a notebook\n",
        "and also allow execution via the command line.\n",
        "\n",
        "## Background\n",
        "\n",
        "To run Auto-sklearn distributed on multiple machines we need to set\n",
        "up three components:\n",
        "\n",
        "1. **Auto-sklearn and a dask client**. This will manage all workload, find new\n",
        "   configurations to evaluate and submit jobs via a dask client. As this\n",
        "   runs Bayesian optimization it should be executed on its own CPU.\n",
        "2. **The dask workers**. They will do the actual work of running machine\n",
        "   learning algorithms and require their own CPU each.\n",
        "3. **The scheduler**. It manages the communication between the dask client\n",
        "   and the different dask workers. As the client and all workers connect\n",
        "   to the scheduler it must be started first. This is a light-weight job\n",
        "   and does not require its own CPU.\n",
        "\n",
        "We will now start these three components in reverse order: scheduler,\n",
        "workers and client. Also, in a real setup, the scheduler and the workers should\n",
        "be started from the command line and not from within a Python file via\n",
        "the ``subprocess`` module as done here (for the sake of having a self-contained\n",
        "example).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import statements\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/peng/opt/anaconda3/envs/autosl/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
            "/home/peng/.local/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.0' currently installed).\n",
            "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
          ]
        }
      ],
      "source": [
        "import multiprocessing\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "import dask.distributed\n",
        "import sklearn.datasets\n",
        "import sklearn.metrics\n",
        "\n",
        "from autosklearn.classification import AutoSklearnClassifier\n",
        "from autosklearn.constants import MULTICLASS_CLASSIFICATION\n",
        "\n",
        "tmp_folder = \"/tmp/autosklearn_parallel_3_example_tmp\"\n",
        "\n",
        "worker_processes = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Setup client-scheduler communication\n",
        "\n",
        "In this examples the dask scheduler is started without an explicit\n",
        "address and port. Instead, the scheduler takes a free port and stores\n",
        "relevant information in a file for which we provided the name and\n",
        "location. This filename is also given to the worker so they can find all\n",
        "relevant information to connect to the scheduler.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scheduler_file_name = \"scheduler-file.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Start scheduler\n",
        "\n",
        "Starting the scheduler is done with the following bash command:\n",
        "\n",
        ".. code:: bash\n",
        "\n",
        "    dask-scheduler --scheduler-file scheduler-file.json --idle-timeout 10\n",
        "\n",
        "We will now execute this bash command from within Python to have a\n",
        "self-contained example:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def cli_start_scheduler(scheduler_file_name):\n",
        "    command = f\"dask-scheduler --scheduler-file {scheduler_file_name} --idle-timeout 10\"\n",
        "    proc = subprocess.run(\n",
        "        command,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        shell=True,\n",
        "        check=True,\n",
        "    )\n",
        "    while proc.returncode is None:\n",
        "        time.sleep(1)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    process_python_worker = multiprocessing.Process(\n",
        "        target=cli_start_scheduler,\n",
        "        args=(scheduler_file_name,),\n",
        "    )\n",
        "    process_python_worker.start()\n",
        "    worker_processes.append(process_python_worker)\n",
        "\n",
        "    # Wait a second for the scheduler to become available\n",
        "    time.sleep(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Start two workers\n",
        "\n",
        "Starting the scheduler is done with the following bash command:\n",
        "\n",
        ".. code:: bash\n",
        "\n",
        "    DASK_DISTRIBUTED__WORKER__DAEMON=False \\\n",
        "        dask-worker --nthreads 1 --lifetime 35 --memory-limit 0 \\\n",
        "        --scheduler-file scheduler-file.json\n",
        "\n",
        "We will now execute this bash command from within Python to have a\n",
        "self-contained example. Please note, that\n",
        "``DASK_DISTRIBUTED__WORKER__DAEMON=False`` is required in this\n",
        "case as dask-worker creates a new process, which by default is not\n",
        "compatible with Auto-sklearn creating new processes in the workers itself.\n",
        "We disable dask's memory management by passing ``--memory-limit`` as\n",
        "Auto-sklearn does the memory management itself.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def cli_start_worker(scheduler_file_name):\n",
        "    command = (\n",
        "        \"DASK_DISTRIBUTED__WORKER__DAEMON=False \"\n",
        "        \"dask-worker --nthreads 1 --lifetime 35 --memory-limit 0 \"\n",
        "        f\"--scheduler-file {scheduler_file_name}\"\n",
        "    )\n",
        "    proc = subprocess.run(\n",
        "        command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True\n",
        "    )\n",
        "    while proc.returncode is None:\n",
        "        time.sleep(1)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    for _ in range(2):\n",
        "        process_cli_worker = multiprocessing.Process(\n",
        "            target=cli_start_worker,\n",
        "            args=(scheduler_file_name,),\n",
        "        )\n",
        "        process_cli_worker.start()\n",
        "        worker_processes.append(process_cli_worker)\n",
        "\n",
        "    # Wait a second for workers to become available\n",
        "    time.sleep(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Creating a client in Python\n",
        "\n",
        "Finally we create a dask cluster which also connects to the scheduler via\n",
        "the information in the file created by the scheduler.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mdask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscheduler_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler_file_name\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/autosl/lib/python3.9/site-packages/distributed/client.py:1018\u001b[0m, in \u001b[0;36mClient.__init__\u001b[0;34m(self, address, loop, timeout, set_as_default, scheduler_file, security, asynchronous, name, heartbeat_interval, serializers, deserializers, extensions, direct_to_workers, connection_limit, **kwargs)\u001b[0m\n\u001b[1;32m   1015\u001b[0m preload_argv \u001b[38;5;241m=\u001b[39m dask\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistributed.client.preload-argv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreloads \u001b[38;5;241m=\u001b[39m preloading\u001b[38;5;241m.\u001b[39mprocess_preloads(\u001b[38;5;28mself\u001b[39m, preload, preload_argv)\n\u001b[0;32m-> 1018\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1019\u001b[0m Client\u001b[38;5;241m.\u001b[39m_instances\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecreate_tasks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReplayTaskClient\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/autosl/lib/python3.9/site-packages/distributed/client.py:1220\u001b[0m, in \u001b[0;36mClient.start\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_started \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1220\u001b[0m     \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/autosl/lib/python3.9/site-packages/distributed/utils.py:431\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m e\u001b[38;5;241m.\u001b[39mis_set():\n\u001b[0;32m--> 431\u001b[0m         \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/autosl/lib/python3.9/site-packages/distributed/utils.py:420\u001b[0m, in \u001b[0;36msync.<locals>.wait\u001b[0;34m(timeout)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    422\u001b[0m         loop\u001b[38;5;241m.\u001b[39madd_callback(cancel)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/autosl/lib/python3.9/threading.py:581\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    579\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 581\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/autosl/lib/python3.9/threading.py:316\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 316\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "client = dask.distributed.Client(scheduler_file=scheduler_file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Start Auto-sklearn\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    X, y = sklearn.datasets.load_breast_cancer(return_X_y=True)\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
        "        X, y, random_state=1\n",
        "    )\n",
        "\n",
        "    automl = AutoSklearnClassifier(\n",
        "        delete_tmp_folder_after_terminate=False,\n",
        "        time_left_for_this_task=30,\n",
        "        per_run_time_limit=10,\n",
        "        memory_limit=2048,\n",
        "        tmp_folder=tmp_folder,\n",
        "        seed=777,\n",
        "        # n_jobs is ignored internally as we pass a dask client.\n",
        "        n_jobs=1,\n",
        "        # Pass a dask client which connects to the previously constructed cluster.\n",
        "        dask_client=client,\n",
        "    )\n",
        "    automl.fit(X_train, y_train)\n",
        "\n",
        "    automl.fit_ensemble(\n",
        "        y_train,\n",
        "        task=MULTICLASS_CLASSIFICATION,\n",
        "        dataset_name=\"digits\",\n",
        "        ensemble_kwargs={\"ensemble_size\": 20},\n",
        "        ensemble_nbest=50,\n",
        "    )\n",
        "\n",
        "    predictions = automl.predict(X_test)\n",
        "    print(automl.sprint_statistics())\n",
        "    print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Wait until all workers are closed\n",
        "\n",
        "This is only necessary if the workers are started from within this python\n",
        "script. In a real application one would start them directly from the command\n",
        "line.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    process_python_worker.join()\n",
        "    for process in worker_processes:\n",
        "        process.join()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "autosl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
