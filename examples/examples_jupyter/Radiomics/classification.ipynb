{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "\n",
        "import radiomics\n",
        "from radiomics import featureextractor\n",
        "\n",
        "import pandas as pd\n",
        "import SimpleITK as sitk\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Feature extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 setup working directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "work_dir = '/home/peng/01_data/02_radiomics/demo'\n",
        "data_dir = os.path.join(work_dir,'data')\n",
        "result_dir = os.path.join(work_dir,'result')\n",
        "\n",
        "\n",
        "tmp_volume = os.path.join(work_dir,'TmpVolumeDir')\n",
        "tmp_mask = os.path.join(work_dir,'TmpMaskDir')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## remove files from temparal directory\n",
        "if os.path.isdir(tmp_volume):\n",
        "    shutil.rmtree(tmp_volume)\n",
        "\n",
        "if os.path.isdir(tmp_mask):\n",
        "    shutil.rmtree(tmp_mask)\n",
        "# --------------------------------------------------------\n",
        "# check if dirs exist\n",
        "dir_list = [result_dir,tmp_volume,tmp_mask]\n",
        "for dir in dir_list:\n",
        "    if not os.path.exists(dir):\n",
        "        os.makedirs(dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for users \n",
        "# specify image nii dir and mask nii dir\n",
        "\n",
        "# Create lists to store the paths to the images and masks\n",
        "image_paths = {}\n",
        "mask_paths = {}\n",
        "\n",
        "# Iterate over the patient directories in data_dir\n",
        "for patient_dir in os.listdir(data_dir):\n",
        "    patient_path = os.path.join(data_dir, patient_dir)\n",
        "    \n",
        "    # Look for 'image.nii.gz' in the patient directory and add its path to image_paths\n",
        "    image_file = glob.glob(os.path.join(patient_path, 'image.nii.gz'))\n",
        "    if image_file:\n",
        "        image_paths[patient_dir] = image_file[0]\n",
        "    \n",
        "    # Look for 'mask.nii.gz' in the p        img = io.read_image_sitk(Path(image_path))\n",
        "        mask = io.read_segmentation_sitk(Path(mask_path), label=label)atient directory and add its path to mask_paths\n",
        "    mask_file = glob.glob(os.path.join(patient_path, 'segmentation.nii.gz'))\n",
        "    if mask_file:\n",
        "        mask_paths[patient_dir] = mask_file[0]\n",
        "\n",
        "# Sort the lists\n",
        "# image_paths.sort()\n",
        "# mask_paths.sort()\n",
        "\n",
        "# Print the paths to the images and masks\n",
        "for patient in image_paths:\n",
        "    print(f'Patient: {patient}')\n",
        "    print('Image:', image_paths[patient])\n",
        "    print('Mask:', mask_paths[patient])\n",
        "\n",
        "# check if they are in the same length\n",
        "#TODO check id and pre/post\n",
        "# check_dataset(volume_list, mask_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create lists to store the paths to the images and masks\n",
        "image_paths = []\n",
        "mask_paths = []\n",
        "\n",
        "# Iterate over the patient directories in data_dir\n",
        "for patient_dir in sorted(os.listdir(data_dir)):\n",
        "    patient_path = os.path.join(data_dir, patient_dir)\n",
        "    \n",
        "    # Look for 'image.nii.gz' in the patient directory and add its path to image_paths\n",
        "    image_file = glob.glob(os.path.join(patient_path, 'image.nii.gz'))\n",
        "    if image_file:\n",
        "        image_paths.append(image_file[0])\n",
        "    \n",
        "    # Look for 'mask.nii.gz' in the patient directory and add its path to mask_paths\n",
        "    mask_file = glob.glob(os.path.join(patient_path, 'segmentation.nii.gz'))\n",
        "    if mask_file:\n",
        "        mask_paths.append(mask_file[0])\n",
        "\n",
        "# Print the paths to the images and masks\n",
        "for i in range(len(image_paths)):\n",
        "    print(f'Image: {image_paths[i]}')\n",
        "    print(f'Mask: {mask_paths[i]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_dataset(list_volume, list_mask):\n",
        "    if len(list_volume) != len(list_mask):\n",
        "        raise ValueError('There exists a mismatch between two datasets.')\n",
        "    \n",
        "# check if they are in the same length\n",
        "#TODO check id and pre/post\n",
        "check_dataset(image_paths, mask_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Tumor delineation check\n",
        "using Dice similarity to calc mask reproducibility, should set a propper cut-off value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dice similarity\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import dice\n",
        "\n",
        "def calculate_dice_scipy(mask1_path, mask2_path):\n",
        "    # Load the masks\n",
        "    mask1 = nib.load(mask1_path).get_fdata()\n",
        "    mask2 = nib.load(mask2_path).get_fdata()\n",
        "\n",
        "    # Binarize the masks\n",
        "    mask1 = np.where(mask1 > 0, 1, 0)\n",
        "    mask2 = np.where(mask2 > 0, 1, 0)\n",
        "\n",
        "    # Flatten the masks\n",
        "    mask1 = mask1.flatten()\n",
        "    mask2 = mask2.flatten()\n",
        "\n",
        "    # Calculate Dice dissimilarity\n",
        "    dice_dissimilarity = dice(mask1, mask2)\n",
        "\n",
        "    # Calculate Dice similarity\n",
        "    dice_similarity = 1. - dice_dissimilarity\n",
        "\n",
        "    return dice_similarity\n",
        "\n",
        "# Usage:\n",
        "# dice = calculate_dice_scipy('path_to_mask1.nii.gz', 'path_to_mask2.nii.gz')\n",
        "# print(dice)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 Feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# setup parameters\n",
        "params = os.path.join('/home/peng/00_github/03_radiomics/240602_auto-sklearn_SP/examples/examples_jupyter/Radiomics/Radiomics_Settings', 'exampleMR_1mm_SV.yaml')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        " # Instantiate a pandas data frame to hold the results of all patients\n",
        "featureVector = pd.DataFrame()\n",
        "sub = pd.DataFrame()\n",
        "# Iterate over the image and mask paths\n",
        "for image_path, mask_path in zip(image_paths, mask_paths):\n",
        "    # Extract the features\n",
        "    # print(f'Extracting features for {image_path}...')\n",
        "    # print(f'Extracting features for {mask_path}...')\n",
        "    img = sitk.ReadImage(image_path)\n",
        "    # img_array = sitk.GetArrayFromImage(img)\n",
        "    mask = sitk.ReadImage(mask_path)\n",
        "    # mask.SetSpacing(img.GetSpacing())\n",
        "    # mask.SetOrigin(img.GetOrigin())\n",
        "    # mask.SetDirection(img.GetDirection())\n",
        "    mask.CopyInformation(img)\n",
        "    # mask_array = sitk.GetArrayFromImage(mask)\n",
        "    # Check if the image and mask match\n",
        "    if img.GetSize() != mask.GetSize() or img.GetSpacing() != mask.GetSpacing() or img.GetOrigin() != mask.GetOrigin():\n",
        "        print(f'Geometry mismatch between image {image_path} and mask {mask_path}')\n",
        "    \n",
        "    result = pd.Series(extractor.execute(img, mask))\n",
        "    featureVector = featureVector._append(result, ignore_index=True)\n",
        "\n",
        "    directories = os.path.normpath(image_path).split(os.sep)\n",
        "    last_directories = directories[-2:-1]\n",
        "    series = pd.Series(last_directories, index=['subject'])\n",
        "    # Append the series to the DataFrame\n",
        "    sub = sub._append(series, ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "featureVector = featureVector.iloc[:, 22:]\n",
        "featureVector.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# readin the labels\n",
        "label_df = pd.read_csv(os.path.join(data_dir, \"labels.csv\"))\n",
        "label_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sub_lable_feature = pd.concat([label_df, featureVector], axis=1)\n",
        "sub_lable_feature.to_csv(os.path.join(result_dir, 'sub_lable_feature.csv'), index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sub_lable_feature = pd.read_csv(os.path.join(result_dir, 'sub_lable_feature.csv'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sub_lable_feature.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "featureVector = sub_lable_feature.iloc[:, 2:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "featureVector.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Preprocessing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 intraclass correlation coefficients (ICCs) from different masks delineated by different doctor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# simulate some random data to demonstrate the feature icc calculation\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "featureVector_std_dev = featureVector.std()\n",
        "featureVector.shape\n",
        "random_df = pd.DataFrame(np.random.rand(*featureVector.shape), columns=featureVector.columns)* featureVector_std_dev\n",
        "featureVector_2_df = featureVector + random_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import pingouin as pg\n",
        "\n",
        "# Ensure both dataframes have the same columns in the same order\n",
        "assert set(featureVector.columns) == set(featureVector_2_df.columns)\n",
        "featureVector = featureVector[featureVector_2_df.columns]\n",
        "\n",
        "# Initialize an empty dictionary to store the ICC values\n",
        "icc_values = {}\n",
        "# Loop over the columns\n",
        "for column in featureVector.columns:\n",
        "    # Create a new dataframe that combines the data from the same column in both dataframes\n",
        "    df_tmp = featureVector[[column]].copy()\n",
        "    df_tmp['rater'] = 'rater1'\n",
        "    df_tmp['subject'] = df_tmp.index\n",
        "    df_2 = featureVector_2_df[[column]].copy()\n",
        "    df_2['rater'] = 'rater2'\n",
        "    df_2['subject'] = df_2.index\n",
        "\n",
        "    df = pd.concat([df_tmp, df_2])\n",
        "    # df[column] = pd.to_numeric(df[column], errors='coerce')\n",
        "    # # Calculate the ICC and store the result in the dictionary\n",
        "    icc = pg.intraclass_corr(data=df, targets='subject', raters='rater', ratings=column,nan_policy=\"omit\")['ICC'].values[2] # ICC ICC(3,1)\n",
        "    icc_values[column] = icc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "icc_df = pd.DataFrame(list(icc_values.items()), columns=['Features', 'ICC'])\n",
        "icc_selected = icc_df[icc_df['ICC'] > 0.965]\n",
        "print(icc_selected.index)\n",
        "print(icc_selected['Features'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_icc_selected = featureVector[icc_selected['Features']]\n",
        "feature_icc_selected.to_csv(os.path.join(result_dir, 'feature_icc_selected.csv'), index=False)\n",
        "# feature_icc_selected['ID'] = merged_feature_df['ID']\n",
        "# feature_icc_selected['diagnosis'] = merged_feature_df['diagnosis']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 variance of each feature value \n",
        "low variance (<75%) will be removed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calculate variance of each feature\n",
        "# low variance (<75%) will be removed\n",
        "\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "# Initialize a VarianceThreshold feature selector\n",
        "selector = VarianceThreshold(threshold=0.1)  # Adjust the threshold as needed\n",
        "\n",
        "# Fit the selector to the data and transform the data\n",
        "feature_icc_var_selected = selector.fit_transform(feature_icc_selected)\n",
        "\n",
        "# Convert back to DataFrame\n",
        "feature_icc_var_selected = pd.DataFrame(feature_icc_var_selected, columns=feature_icc_selected.columns[selector.get_support()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3 chi2 of each feature value \n",
        "best 10 feature based chi2 was selected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['original_shape_Maximum3DDiameter',\n",
            "       'log-sigma-1-0-mm-3D_glszm_LargeAreaHighGrayLevelEmphasis',\n",
            "       'log-sigma-1-0-mm-3D_glszm_SizeZoneNonUniformity',\n",
            "       'log-sigma-3-0-mm-3D_glrlm_RunLengthNonUniformity',\n",
            "       'log-sigma-5-0-mm-3D_firstorder_10Percentile',\n",
            "       'log-sigma-5-0-mm-3D_firstorder_Maximum',\n",
            "       'wavelet-HH_glrlm_ShortRunHighGrayLevelEmphasis',\n",
            "       'wavelet-HH_glszm_SmallAreaHighGrayLevelEmphasis',\n",
            "       'wavelet-HH_glszm_ZoneVariance', 'wavelet-LL_firstorder_Median'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "# Find the minimum value in the DataFrame\n",
        "min_value = feature_icc_var_selected.min().min()\n",
        "# If the minimum value is less than 0, shift all values by its absolute value\n",
        "if min_value < 0:\n",
        "    feature_icc_var_selected += abs(min_value)\n",
        "\n",
        "selector = SelectKBest(chi2, k=10)\n",
        "\n",
        "feature_icc_var_chi2_selected = selector.fit_transform(feature_icc_var_selected, sub_lable_feature['diagnosis'])\n",
        "feature_icc_var_chi2_selected.shape\n",
        "feature_icc_var_chi2_selected = pd.DataFrame(feature_icc_var_chi2_selected, columns=feature_icc_var_selected.columns[selector.get_support()])\n",
        "print(feature_icc_var_chi2_selected.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.4 mutual information of each feature value \n",
        "best 10 feature based  mutual information was selected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: Index(['original_shape_Maximum3DDiameter',\n",
            "       'log-sigma-1-0-mm-3D_firstorder_RobustMeanAbsoluteDeviation',\n",
            "       'log-sigma-1-0-mm-3D_glszm_LargeAreaHighGrayLevelEmphasis',\n",
            "       'log-sigma-1-0-mm-3D_glszm_SizeZoneNonUniformity',\n",
            "       'log-sigma-3-0-mm-3D_glrlm_RunLengthNonUniformity',\n",
            "       'log-sigma-5-0-mm-3D_firstorder_10Percentile',\n",
            "       'wavelet-HH_firstorder_MeanAbsoluteDeviation',\n",
            "       'wavelet-HH_glszm_GrayLevelVariance',\n",
            "       'wavelet-HH_gldm_LargeDependenceLowGrayLevelEmphasis',\n",
            "       'wavelet-LL_firstorder_Median'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "\n",
        "# Initialize a SelectKBest feature selector\n",
        "selector = SelectKBest(mutual_info_classif, k=10)  # Adjust the number of features as needed\n",
        "\n",
        "# Fit the selector to the data and transform the data\n",
        "feature_selected = selector.fit_transform(feature_icc_var_selected, sub_lable_feature['diagnosis'])\n",
        "\n",
        "# Convert back to DataFrame\n",
        "feature_icc_var_mi_selected = pd.DataFrame(feature_selected, columns=feature_icc_var_selected.columns[selector.get_support()])\n",
        "\n",
        "# Get the names of the selected features\n",
        "selected_features = feature_icc_var_selected.columns[selector.get_support()]\n",
        "\n",
        "print('Selected features:', selected_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.5 variance inflation factor (VIF)\n",
        "\n",
        "If you want to select features that are least correlated with each other, you can use the concept of variance inflation factor (VIF). VIF measures the correlation and multicollinearity of features. A high VIF value for a feature means that feature is highly correlated with other features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: 4     log-sigma-1-0-mm-3D_glszm_LargeAreaHighGrayLev...\n",
            "15                        wavelet-HH_glszm_ZoneVariance\n",
            "5       log-sigma-1-0-mm-3D_glszm_SizeZoneNonUniformity\n",
            "6      log-sigma-3-0-mm-3D_glrlm_RunLengthNonUniformity\n",
            "17                         wavelet-LL_firstorder_Median\n",
            "0                      original_shape_Maximum3DDiameter\n",
            "8                log-sigma-5-0-mm-3D_firstorder_Maximum\n",
            "7           log-sigma-5-0-mm-3D_firstorder_10Percentile\n",
            "12       wavelet-HH_glrlm_ShortRunHighGrayLevelEmphasis\n",
            "14      wavelet-HH_glszm_SmallAreaHighGrayLevelEmphasis\n",
            "Name: features, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# calculate VIF between features\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Calculate VIF for each feature\n",
        "vif = pd.DataFrame()\n",
        "vif[\"VIF Factor\"] = [variance_inflation_factor(feature_icc_var_selected.values, i) for i in range(feature_icc_var_selected.shape[1])]\n",
        "vif[\"features\"] = feature_icc_var_selected.columns\n",
        "\n",
        "# Sort by VIF Factor in ascending order\n",
        "vif = vif.sort_values(\"VIF Factor\")\n",
        "\n",
        "# Select the features with the lowest VIF\n",
        "selected_features = vif[\"features\"].head(10)  # Adjust the number of features as needed\n",
        "\n",
        "print('Selected features:', selected_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.6 Pearson correlation coefficient\n",
        "large correlation coefficient (>0.75) will be removed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.7 t-test or Mann-Whitney U test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# t-test between groups or Mann-Whitney U test\n",
        "from scipy.stats import mannwhitneyu,ttest_ind,shapiro\n",
        "\n",
        "# Define an empty list to hold the selected features\n",
        "selected_features = []\n",
        "\n",
        "# Perform the Mann-Whitney U test for each feature\n",
        "for feature in feature_icc_var_selected.columns:\n",
        "    group1 = feature_icc_var_selected.loc[sub_lable_feature['diagnosis'] == 0, feature]\n",
        "    group2 = feature_icc_var_selected.loc[sub_lable_feature['diagnosis'] == 1, feature]\n",
        "    \n",
        "    # Test the normality of the feature in each group\n",
        "    _, p1 = shapiro(group1)\n",
        "    _, p2 = shapiro(group2)\n",
        "    \n",
        "    # If both groups are normally distributed, perform the t-test\n",
        "    if p1 > 0.05 and p2 > 0.05:\n",
        "        stat, p = ttest_ind(group1, group2)\n",
        "    # Otherwise, perform the Mann-Whitney U test\n",
        "    else:\n",
        "        stat, p = mannwhitneyu(group1, group2)\n",
        "    \n",
        "    # If the p-value is less than 0.05, add the feature to the list of selected features\n",
        "    if p < 0.05:\n",
        "        selected_features.append(feature)\n",
        "\n",
        "print('Selected features:', selected_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Classification\n",
        "\n",
        "The following example shows how to fit a simple classification model with\n",
        "*auto-sklearn*.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "import sklearn.datasets\n",
        "import sklearn.metrics\n",
        "from sklearn import model_selection\n",
        "import autosklearn.classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y = sklearn.datasets.load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
        "    X, y, random_state=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build and fit a classifier\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "automl = autosklearn.classification.AutoSklearnClassifier(\n",
        "    time_left_for_this_task=120,\n",
        "    per_run_time_limit=30,\n",
        "    memory_limit=16384,\n",
        "    tmp_folder=\"/tmp/autosklearn_classification_example_tmp\",\n",
        ")\n",
        "automl.fit(X_train, y_train, dataset_name=\"breast_cancer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## View the models found by auto-sklearn\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(automl.leaderboard())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Print the final ensemble constructed by auto-sklearn\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pprint(automl.show_models(), indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get the Score of the final ensemble\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "predictions = automl.predict(X_test)\n",
        "print(\"Accuracy score:\", sklearn.metrics.accuracy_score(y_test, predictions))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "autosl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
